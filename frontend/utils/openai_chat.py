from openai import OpenAI
import os
import json
from dotenv import load_dotenv

load_dotenv()

# OpenAI í˜¸í™˜ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1"
)

def chat_with_upstage(messages, model="solar-pro2-preview", stream=False, reasoning_effort="medium"):
    """
    Upstage OpenAI í˜¸í™˜ APIë¥¼ ì‚¬ìš©í•œ ì±„íŒ… í•¨ìˆ˜
    
    Args:
        messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "user", "content": "..."}]
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: solar-pro2-preview)
        stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        reasoning_effort: ì¶”ë¡  ê°•ë„ ("low", "medium", "high")
    
    Returns:
        ì‘ë‹µ í…ìŠ¤íŠ¸ ë˜ëŠ” ìŠ¤íŠ¸ë¦¼ ê°ì²´
    """
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            reasoning_effort=reasoning_effort,
            stream=stream,
        )
        
        if stream:
            return response
        else:
            return response.choices[0].message.content
            
    except Exception as e:
        print(f"ì±„íŒ… API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def summarize_conversation_history(chat_history):
    """
    ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        chat_history: ìš”ì•½í•  ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ìš”ì•½ëœ ëŒ€í™” ë‚´ìš©
    """
    if not chat_history:
        return ""
    
    # ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    conversation_text = ""
    for msg in chat_history:
        if msg["role"] in ["user", "assistant"]:
            role_name = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
            content = msg["content"]
            # íŠ¹ìˆ˜ í˜•ì‹ ë©”ì‹œì§€ ì œì™¸
            if not content.startswith("ğŸ“„") and not content.startswith("âŒ"):
                conversation_text += f"{role_name}: {content}\n\n"
    
    if not conversation_text.strip():
        return ""
    
    # ìš”ì•½ ìƒì„±
    messages = [
        {
            "role": "system",
            "content": """ë‹¹ì‹ ì€ ëŒ€í™” ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ëŒ€í™” ë‚´ìš©ì„ ê°„ê²°í•˜ê³  í•µì‹¬ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

**ì´ì „ ëŒ€í™” ìš”ì•½:**
- ì£¼ìš” ì§ˆë¬¸ê³¼ ë‹µë³€ ë‚´ìš©ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
- ì¤‘ìš”í•œ ë§¥ë½ì´ë‚˜ ì •ë³´ë¥¼ í¬í•¨
- 3-5ê°œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±

ëŒ€í™”ì˜ íë¦„ê³¼ í•µì‹¬ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."""
        },
        {
            "role": "user",
            "content": f"ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{conversation_text}"
        }
    ]
    
    try:
        summary = chat_with_upstage(messages, reasoning_effort="medium")
        return summary if summary else ""
    except:
        return ""

def build_conversation_messages(chat_history, system_prompt, current_input, recent_count=7):
    """
    ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ë©”ì‹œì§€ êµ¬ì„± (ìµœê·¼ 7ê°œëŠ” ê·¸ëŒ€ë¡œ, ì´ì „ ê²ƒë“¤ì€ ìš”ì•½)
    
    Args:
        chat_history: Streamlit ì„¸ì…˜ì˜ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        current_input: í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        recent_count: ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìµœê·¼ ëŒ€í™” ìˆ˜ (ê¸°ë³¸ê°’: 7)
    
    Returns:
        OpenAI í˜•ì‹ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    messages = [{"role": "system", "content": system_prompt}]
    
    # ëŒ€í™” ê¸°ë¡ì´ recent_count*2 ê°œë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°
    if len(chat_history) > recent_count * 2:
        # ì´ì „ ëŒ€í™”ë“¤ (ìš”ì•½ ëŒ€ìƒ)
        old_history = chat_history[:-recent_count*2]
        # ìµœê·¼ ëŒ€í™”ë“¤ (ê·¸ëŒ€ë¡œ ìœ ì§€)
        recent_history = chat_history[-recent_count*2:]
        
        # ì´ì „ ëŒ€í™” ìš”ì•½ ìƒì„±
        conversation_summary = summarize_conversation_history(old_history)
        
        # ìš”ì•½ì´ ìˆìœ¼ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ì— ì¶”ê°€
        if conversation_summary:
            enhanced_system_prompt = f"""{system_prompt}

{conversation_summary}

ìœ„ëŠ” ì´ì „ ëŒ€í™”ì˜ ìš”ì•½ì…ë‹ˆë‹¤. ì´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""
            messages[0]["content"] = enhanced_system_prompt
        
        # ìµœê·¼ ëŒ€í™”ë§Œ ë©”ì‹œì§€ì— ì¶”ê°€
        target_history = recent_history
    else:
        # ëŒ€í™”ê°€ ì ìœ¼ë©´ ëª¨ë“  ëŒ€í™” ìœ ì§€
        target_history = chat_history
    
    # ëŒ€í™” ê¸°ë¡ì„ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    for msg in target_history:
        if msg["role"] in ["user", "assistant"]:
            content = msg["content"]
            # íŠ¹ìˆ˜ í˜•ì‹ ë©”ì‹œì§€ ì œì™¸
            if not content.startswith("ğŸ“„") and not content.startswith("âŒ"):
                messages.append({
                    "role": msg["role"],
                    "content": content
                })
    
    # í˜„ì¬ ì…ë ¥ ì¶”ê°€
    messages.append({"role": "user", "content": current_input})
    
    return messages

def summarize_document(text, language="Korean"):
    """
    ë¬¸ì„œ ìš”ì•½ í•¨ìˆ˜
    
    Args:
        text: ìš”ì•½í•  í…ìŠ¤íŠ¸
        language: ìš”ì•½ ì–¸ì–´ (ê¸°ë³¸ê°’: Korean)
    
    Returns:
        ìš”ì•½ëœ í…ìŠ¤íŠ¸
    """
    messages = [
        {
            "role": "system",
            "content": f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ {language}ë¡œ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ìš”ì•½ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ì£¼ìš” ë‚´ìš©
- í•µì‹¬ í¬ì¸íŠ¸ë“¤ì„ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ ì •ë¦¬

## ê²°ë¡ 
- ë¬¸ì„œì˜ ì£¼ìš” ê²°ë¡ ì´ë‚˜ ì‹œì‚¬ì 

ìš”ì•½ì€ ì›ë¬¸ì˜ 20% ì´ë‚´ ê¸¸ì´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        },
        {
            "role": "user", 
            "content": f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text}"
        }
    ]
    
    return chat_with_upstage(messages, reasoning_effort="high")

def answer_question_with_memory(question, chat_history, context=None):
    """
    ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ ì§ˆë¬¸ ë‹µë³€ í•¨ìˆ˜
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡
        context: ì°¸ê³ í•  ë¬¸ë§¥ (ì„ íƒì‚¬í•­)
    
    Returns:
        ë‹µë³€ í…ìŠ¤íŠ¸
    """
    if context:
        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë¬¸ë§¥ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ë§¥: {context}"""
    else:
        system_prompt = """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.
ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³  ì—°ì†ì„± ìˆëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ëª¨ë¥´ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”."""
    
    messages = build_conversation_messages(chat_history, system_prompt, question)
    return chat_with_upstage(messages, reasoning_effort="high")

def document_based_qa_with_memory(document_summary, user_question, chat_history):
    """
    ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ í•¨ìˆ˜
    
    Args:
        document_summary: ë¬¸ì„œ ìš”ì•½
        user_question: ì‚¬ìš©ì ì§ˆë¬¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡
    
    Returns:
        ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
    """
    system_prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ë‚´ìš©ê³¼ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ë‹µë³€
[êµ¬ì²´ì ì¸ ë‹µë³€ ë‚´ìš©]

## ê·¼ê±°
[ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¶€ë¶„]

## ì¶”ê°€ ì •ë³´
[ê´€ë ¨ëœ ì¶”ê°€ ì •ë³´ë‚˜ ì œì•ˆì‚¬í•­]

ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•´ì£¼ì„¸ìš”.

ì°¸ì¡° ë¬¸ì„œ ìš”ì•½:
{document_summary}"""
    
    messages = build_conversation_messages(chat_history, system_prompt, user_question)
    return chat_with_upstage(messages, reasoning_effort="high")

def stream_chat_response_with_memory(chat_history, system_prompt, current_input):
    """
    ëŒ€í™” ê¸°ë¡ì„ ê³ ë ¤í•œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ í•¨ìˆ˜
    
    Args:
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        current_input: í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
    
    Yields:
        ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì²­í¬
    """
    try:
        messages = build_conversation_messages(chat_history, system_prompt, current_input)
        stream = chat_with_upstage(messages, stream=True, reasoning_effort="medium")
        
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content
                
    except Exception as e:
        yield f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
def answer_question(question, context=None):
    """
    ì§ˆë¬¸ ë‹µë³€ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì—†ìŒ - í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return answer_question_with_memory(question, [], context)

def document_based_qa(document_summary, user_question):
    """
    ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì—†ìŒ - í•˜ìœ„ í˜¸í™˜ì„±)
    """
    return document_based_qa_with_memory(document_summary, user_question, [])

def stream_chat_response(messages):
    """
    ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ í•¨ìˆ˜ (ë©”ëª¨ë¦¬ ì—†ìŒ - í•˜ìœ„ í˜¸í™˜ì„±)
    """
    try:
        stream = chat_with_upstage(messages, stream=True, reasoning_effort="medium")
        
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content
                
    except Exception as e:
        yield f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}" 